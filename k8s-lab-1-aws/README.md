# Passo a passo para pr√°tica de provisionar k8s com CAPI na AWS

**OBJETIVO**: instanciar uma m√°quina EC2 na AWS e executar nela um cluster Kubernetes via kind, utilizado como management cluster para trabalhar com CRDs do Cluster API (CAPI).

**FERRAMENTAS** : Para isso usaremos `terraform` para provisionar e configurar recursos rapidamente, `ssh-keygen` para gerar chaves *ssh*, `AWS CLI` para dar comandos √† AWS e `OpenSSH` como cliente *ssh* para se conectar ao EC2.

## 1 ‚Äì Cria√ß√£o da chave SSH local

O `terraform` √© um utilit√°rio de linha de comando usado para configurar e provisionar recursos de forma declarativa. Entretanto, essa CLI n√£o oferece suporte a download. Ou seja, √© poss√≠vel us√°-lo para criar uma chave de acesso SSH para uma inst√¢ncia EC2, mas a ferramenta n√£o a baixa automaticamente, o que torna o processo in√∫til nesse contexto.

Ent√£o, vamos gerar o par de chaves localmente e export√°-lo para uso no EC2. Usando o utilit√°rio `ssh-keygen`, normalmente dispon√≠vel em sistemas Unix, geraremos as chaves que ser√£o usadas para acesso SSH √† inst√¢ncia EC2. Por favor, **navegue at√© a raiz do projeto** e d√™ o seguinte comando ‚Äî obs.: n√£o utilize nenhuma `passphrase`:

```bash
ssh-keygen -t ed25519 -f .aws/ec2-keys/k8s-bootstrap-lab-key.pem
```
Isso cria dois arquivos:
- aws/ec2-keys/k8s-bootstrap-lab-key.pem        ‚Üí chave privada (fica somente na m√°quina local)
- aws/ec2-keys/k8s-bootstrap-lab-key.pem.pub    ‚Üí chave p√∫blica (enviada √† AWS para compor o Key Pair)

N√£o coloque nenhuma frase nem nada, s√≥ d√™ enter 'vazio' em tudo.

## 2 ‚Äì Provisionar a inst√¢ncia EC2 com Terraform

Agora navegue at√© a pasta *k8s-lab-1-aws/terraform*. Nessa pasta, preciso que fa√ßa a [Configura√ß√£o das credenciais AWS](../.aws/README.md#configura√ß√£o-das-credenciais-aws).

Inicializar o Terraform:
```bash
terraform init
```

Ver o plano:
```bash
terraform plan
```

Aplicar:
```bash
terraform apply
```

Depois, acessar via SSH:
```bash
ssh -i aws/ec2-keys/k8s-bootstrap-lab-key ubuntu@<IP_PUBLICO_EC2>
```

√â importante, ap√≥s as pr√°ticas, remover toda a infraestrutura criada para evitar custos desnecess√°rios. Isso pode ser feito com o comando abaixo. Aten√ß√£o: ele remove **todos** os recursos gerenciados pelo Terraform no diret√≥rio atual. Tudo o que foi criado a partir da se√ß√£o 3 ser√° apagado.

```bash
terraform destroy
```


## 3 ‚Äî Configurar a inst√¢ncia EC2 para virar o "Bootstrap Cluster"

`Bootstrap Cluster` √© o nome dado ao cluster que tem a capacidade de criar outros clusters, isto √©, ele serve para fazer o bootstrap de outros. Ele n√£o serve para outras quest√µes. Comumente, o que se faz √© instalar uma vers√£o reduzida do k8s s√≥ para instalar a camada do CAPI em cima. 

Para acessar a inst√¢ncia criada, voc√™ pode ir at√© `EC2 > Instances > **id_sua_instancia** > Connect to instance`. Ver√° um Exemplo de comando de conex√£o ssh, algo assim: `ssh -i "k8s-bootstrap-lab-key.pem" ubuntu@ec2-34-201-148-231.compute-1.amazonaws.com`. Voc√™ precisa navegar at√© a raiz do projeto e dar esses comandos:

```bash
cd .aws/ec2-keys/
ssh -i "k8s-bootstrap-lab-key.pem" ubuntu@ec2-34-201-148-231.compute-1.amazonaws.com
```

> üîé **NOTA** : a parte *ubuntu@ec2-34-201-148-231.compute-1.amazonaws.com* √© din√¢mica e atribu√≠da pela AWS, pois est√° vinculada ao IP p√∫blico da inst√¢ncia. Ela muda quando a inst√¢ncia √© **parada** e **iniciada** novamente (ou **recriada**).

<p align="center"><img src="../docs/images/image1.png" width="500"><br><em>Onde encontrar o SSH de conex√£o no EC2</em></p>

Da√≠, fa√ßa as coisas coisas triviais:

> üîé **NOTA** : como op√ß√£o, eu implementei um script de configura√ß√£o da inst√¢ncia e voc√™ pode saber como usa-lo em [Configure sua M√°quina com um Script](bootstrap.md#configure-sua-m√°quina-com-um-script) ‚Äî mas recomendo que fa√ßa manualmente pelo menos uma vez para saber o que est√° rolando.

1. Atualizar o sistema e instalar algumas coisas:
```
sudo apt update && sudo apt upgrade -y
```

2. **Instalar Docker** (Kind precisa disso):
```bash
sudo apt install -y docker.io        # Instala o Docker a partir dos reposit√≥rios da distribui√ß√£o
sudo systemctl enable --now docker   # Habilita o servi√ßo do Docker e inicia imediatamente
sudo usermod -aG docker ubuntu       # Adiciona o usu√°rio 'ubuntu' ao grupo docker para executar comandos sem sudo
newgrp docker                        # Recarrega os grupos do usu√°rio e aplica o acesso ao Docker.
``` 

> üîé **NOTA** : Aqui √© bom entrar e sair de novo da conex√£o SSH, para for√ßar as permiss√µes. D√™ o comando `exit` e depois acesse a inst√¢ncia novamente.

3. **Instalar Kind**

O Kind √© `Kubernetes In Docker`

```bash
# Baixa o bin√°rio do KIND (Kubernetes in Docker) para Linux
curl -Lo kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64

# Torna o arquivo baixado execut√°vel
chmod +x kind

# Move o bin√°rio para um diret√≥rio presente no PATH do sistema
# para que o comando 'kind' possa ser executado de qualquer lugar
sudo mv kind /usr/local/bin/

# Verifica se o KIND foi instalado corretamente e exibe a vers√£o
kind --version
```

4. **Instalar kubectl**

`kubectl` √© s√≥ o cliente que fala com o Kubernetes, √© um CLI.

```bash
# Baixa a vers√£o est√°vel mais recente do kubectl para Linux (amd64)
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

# Torna o bin√°rio do kubectl execut√°vel
chmod +x kubectl

# Move o kubectl para um diret√≥rio presente no PATH do sistema
# permitindo executar o comando de qualquer lugar
sudo mv kubectl /usr/local/bin/

# Verifica se o kubectl foi instalado corretamente
# e exibe a vers√£o do cliente
kubectl version --client
```

5. Criar o cluster Kind que ser√° o management cluster
```bash
# Cria um cluster Kubernetes local usando KIND
# O cluster ser√° executado dentro de containers Docker e receber√° o nome 'capi-mgmt'
kind create cluster --name capi-mgmt

# Lista os Nodes registrados no cluster rec√©m-criado
# permitindo verificar se o control plane est√° ativo
kubectl get nodes
```

## 4 ‚Äî Inicializa√ß√£o do Cluster API no cluster de gerenciamento (Kind)

Ta, agora eu vou explicar onde voc√™ est√° neste momento de uma forma mais ou menos visual:

```bash
Seu Desktop # Sua m√°quina local.
   .
   .
   .   (conex√£o SSH)
   .
   .
EC2 (t3.micro)
# Uma m√°quina virtual fornecida pela AWS.
# Para todos os efeitos, √© um computador remoto rodando Linux.
# Tudo o que vem abaixo depende dela estar ligada.

‚îî‚îÄ‚îÄ Docker
    # Um programa que roda continuamente nessa m√°quina (um daemon).
    # Ele permite executar aplica√ß√µes isoladas chamadas containers.
    # Esses containers compartilham o kernel da EC2, mas t√™m processos,
    # rede e filesystem isolados.

    ‚îî‚îÄ‚îÄ Container KIND
        # Um container criado pelo Docker.
        # Ele n√£o √© o Kubernetes em si.
        # Ele existe para CRIAR e HOSPEDAR um cluster Kubernetes usando containers.
        # Pense nele como um "ambiente controlado" onde o Kubernetes vai rodar.

        ‚îî‚îÄ‚îÄ Kubernetes
            # Agora sim, o sistema Kubernetes.
            # Ele √© composto por v√°rios processos que, juntos,
            # formam um cluster capaz de gerenciar aplica√ß√µes distribu√≠das.

            ‚îú‚îÄ‚îÄ control-plane
            # N√ÉO √© um node de execu√ß√£o.
            # √â o conjunto de componentes que controlam o cluster.
            # Ele:
            # - recebe comandos do usu√°rio (kubectl ‚Üí API Server)
            # - decide em qual node cada aplica√ß√£o deve rodar (Scheduler)
            # - garante que o estado desejado seja mantido (Controllers)
            # Sem o control-plane, o cluster n√£o toma decis√µes.

            ‚îú‚îÄ‚îÄ etcd
            # O banco de dados do Kubernetes.
            # Ele armazena o estado atual e desejado do cluster.
            # O control-plane consulta e atualiza o etcd o tempo todo.
            # Se o etcd √© perdido, o cluster perde sua mem√≥ria.

            ‚îî‚îÄ‚îÄ nodes (virtuais)
            # S√£o os locais onde as aplica√ß√µes realmente rodam.
            # Do ponto de vista do Kubernetes, s√£o m√°quinas.
            # Neste caso, s√£o apenas containers Docker simulando m√°quinas.
            # √â neles que os pods s√£o executados.

```

Em um desenho mais visual ficaria assim:

```mermaid
graph TD
    A[Seu Desktop] -->|SSH| B[EC2 t3.micro]

    B --> C[Docker Daemon]
    C --> D[Container KIND]
    D --> E[Kubernetes Cluster]

    E --> E1[Control Plane]
    E --> E2[etcd]
    E --> E3[Nodes Virtuais]
```


Agora, voc√™ vai adiconar mais uma camada de abstra√ß√£o de software em cima disso kkk ... √© o tal do **Cluster API** e assim teremos a seguinte estrutura:



```mermaid
graph TD
    A[Seu Desktop] -->|SSH| B[EC2 t3.micro]

    B --> C[Docker Runtime]
    C --> D[Container KIND]
    D --> E[Kubernetes Management Cluster]

    E --> E1[Control Plane]
    E --> E2[etcd]
    E --> E3[Nodes Virtuais]

    E --> F[Cluster API CAPI]
    F --> F1[CRDs Cluster Machine MachineSet]
    F --> F2[Controllers de Reconciliacao]
```


O que precisamos fazer agora √© instalar o `clusterctl`. Ele √© s√≥ um CLI que ir√° trocar uma ideia com o k8s dentro do container e pedir para instalar os CRD's do CAPI.

```bash
# Baixa o bin√°rio do clusterctl (CLI do Cluster API) direto do GitHub
curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.7.2/clusterctl-linux-amd64 -o clusterctl

# Torna o arquivo execut√°vel
chmod +x clusterctl

# Move o bin√°rio para um diret√≥rio do PATH
# Assim o comando `clusterctl` pode ser executado de qualquer lugar
sudo mv clusterctl /usr/local/bin/
```

> üîé **NOTA** : `kubectl` √© o cliente gen√©rico do Kubernetes. Fala com o API Server. Cria, l√™ e altera recursos Kubernetes. Inclui CRDs do CAPI, mas n√£o sabe inicializar nada. J√° o `clusterctl` √© o cliente espec√≠fico do Cluster API, ele instala os CRDs do CAPI, instala providers (que √© uma coisa que ainda n√£o discutimos) e validar vers√µes e compatibilidade... √â tipo um instalador, depois que usar ele pode voltar a usar kubectl.

> ‚≠ê **CONCEITO IMPORTANTE**:
Ao adicionar CRDs e controllers ao Kubernetes, voc√™ amplia o escopo do que ele consegue gerenciar. 
O Kubernetes deixa de orquestrar apenas aplica√ß√µes (pods, services, deployments) e passa a atuar como um plano de controle capaz de declarar, criar e manter recursos de cloud utilizados por essas aplica√ß√µes.


Mas antes, os controllers do Cluster API com provider AWS (CAPA) v√£o rodar dentro da EC2 e precisam criar recursos na AWS. Para isso, eles precisam de credenciais v√°lidas da AWS. Como voc√™ sabe, essas credenciais existem no seu desktop e n√£o existem no EC2. Logo, voc√™ precisa colocar l√°.

Ent√£o, volte na raiz do nosso projeto e d√™ os seguintes comandos:

```bash
# Acessa a EC2 via SSH
# Cria o diret√≥rio ~/.aws
# Esse √© o local padr√£o onde ferramentas e SDKs da AWS procuram credenciais
ssh -i .aws/ec2-keys/k8s-bootstrap-lab-key.pem ubuntu@ec2-< DIFERENTE >.compute-1.amazonaws.com "mkdir -p ~/.aws"

# Copia o arquivo credentials do seu desktop
# Envia para a EC2
# Coloca exatamente no caminho esperado pelas ferramentas da AWS
scp -i .aws/ec2-keys/k8s-bootstrap-lab-key.pem .aws/credentials ubuntu@ec2-< DIFERENTE >.compute-1.amazonaws.com:~/.aws/credentials
```

Agora sim, dentro da inst√¢ncia, fa√ßa isso:

```bash
# L√™ o arquivo de credenciais da AWS. Codifica em base64
# Exporta como vari√°vel de ambiente para o clusterctl usar
export AWS_B64ENCODED_CREDENTIALS=$(base64 -w0 ~/.aws/credentials)

# Inicializa o Cluster API no cluster Kubernetes atual
# Instala CRDs do CAPI
# Instala controllers
# Instala o provider de infraestrutura da AWS (CAPA)
clusterctl init --infrastructure aws

# Lista todos os pods em todos os namespaces
# Usado aqui para verificar se os controllers do CAPI/CAPA est√£o rodando
kubectl get pods -A

# Lista os CRDs instalados no cluster
# Filtra para mostrar os recursos relacionados ao Cluster API
kubectl get crds | grep cluster
```

Com isso, voc√™ instalou uma cama extra em cima do CAPI. Voc√™ instalou o CAPA, que d√° ao CAPI o poder de criar recursos na AWS. Com isso voc√™ pode criar uma outra inst√¢ncia inteira e instalar Kubernetes nela, por exemplo. Pode tamb√©m subir servi√ßos no ECS e trata-los como pods.

Na pr√°tica, seu EC2 agora √© oficialmente um ‚Äúorquestrador de clusters Kubernetes‚Äù.


```mermaid
graph TD
    A[Seu Desktop] -->|SSH| B[EC2 t3.micro]

    B --> C[Docker Runtime]
    C --> D[Container KIND]
    D --> E[Kubernetes Management Cluster]

    E --> E1[Control Plane]
    E --> E2[etcd]
    E --> E3[Nodes Virtuais]

    E --> F[Cluster API CAPI]
    F --> F1[CRDs Cluster Machine MachineSet]
    F --> F2[Controllers de Reconciliacao]

    F --> G[Infrastructure Provider CAPA AWS]
    G --> G1[CRDs AWSCluster AWSMachine]
    G --> G2[Controllers AWS EC2 ELB]
```

## 5 ‚Äî Crie seu primeiro Workload

```bash
export AWS_REGION="us-east-1"
export AWS_SSH_KEY_NAME="k8s-bootstrap-lab-key"
export AWS_CONTROL_PLANE_MACHINE_TYPE="t3.medium"
export AWS_NODE_MACHINE_TYPE="t3.medium"
```

Para o apply dos manifests dentro do cluster de gerenciamento.

```bash
clusterctl generate cluster meu-cluster --kubernetes-version v1.28.5 | kubectl apply -f -
```


Quer ver se ele j√° come√ßou? Roda:

```bash
kubectl get clusters
kubectl get awsclusters
kubectl get machines
kubectl get awsmachines
```